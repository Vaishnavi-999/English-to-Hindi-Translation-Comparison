# ğŸŒ Englishâ€“Hindi Translation Projects (NLP)

This repository showcases **two end-to-end NLP translation projects** built using the **IIT Bombay Englishâ€“Hindi dataset** from Hugging Face. The projects cover the **complete lifecycle** of an NLP system â€” from **model building and fine-tuning** to **API deployment** using **FastAPI** and **Hugging Face Spaces**.

---

## ğŸš€ Projects Overview

| Project | Model Type | Description | Live API |
|-------|-----------|-------------|---------|
| **Project 1** | LSTM + Encoderâ€“Decoder | Classic Seq2Seq neural machine translation | âœ… Available |
| **Project 2** | Fine-tuned Transformer | Improved translation using fine-tuning & data polishing | âœ… Available |

---

## ğŸ“Œ Project 1: English â†’ Hindi Translation (LSTM Encoderâ€“Decoder)

### ğŸ”¹ Description
This project implements a **Neural Machine Translation (NMT)** system using:
- **LSTM-based Encoderâ€“Decoder architecture**
- **Sequence-to-Sequence learning**
- **Teacher Forcing** during training

âš ï¸ **Important Note on Performance**  
This model is **trained from scratch** on the IITB dataset without using any pre-trained language knowledge. As a result:
- The model **does not provide very accurate or fluent translations**
- Accuracy is **relatively low** compared to modern approaches
- Translations may sound **grammatically incorrect or incomplete**

ğŸ‘‰ This limitation is **intentional and educational**, as the goal of Project 1 is to understand **core NLP and Seq2Seq fundamentals**, not to achieve production-level accuracy.

### ğŸ”¹ Dataset
- **IIT Bombay Englishâ€“Hindi Parallel Corpus**
- Source: Hugging Face
- Used for supervised sequence-to-sequence learning

### ğŸ”¹ Architecture
```
English Sentence
      â†“
 Tokenization
      â†“
 LSTM Encoder â†’ Context Vector â†’ LSTM Decoder
      â†“
 Hindi Sentence
```

### ğŸ”¹ Key Features
- Text preprocessing (cleaning, tokenization, padding)
- Separate encoder and decoder models
- Separate training and inference logic
- Demonstrates limitations of training from scratch

### ğŸ”¹ Tech Stack
- Python
- TensorFlow / Keras
- NumPy, Pickle
- FastAPI
- Hugging Face Spaces

### ğŸ”¹ Live API
ğŸ”— **English â†’ Hindi Translation API (Baseline Model)**  
ğŸ‘‰ https://huggingface.co/spaces/patil1203/english_to_hindi_translation

---


## ğŸ“Œ Project 2: Fine-Tuned Englishâ€“Hindi Translator (Improved Model)

### ğŸ”¹ Description
This project is an **enhanced and production-ready version** of Project 1. Instead of training from scratch, this model uses **fine-tuning on a pre-trained translation model**, combined with **dataset polishing and optimization**.

âœ… **Key Result**  
- Fine-tuning enables the model to leverage **pre-learned language representations**
- Translation quality improves **significantly**
- Achieves **~95%+ translation accuracy** (approx.) on validation data
- Outputs are **more fluent, grammatically correct, and context-aware**

This clearly demonstrates **why fine-tuning outperforms training from scratch** in real-world NLP applications.

### ğŸ”¹ Improvements Over Project 1
- Uses pre-trained transformer-based model
- Fine-tuned on IITB Englishâ€“Hindi dataset
- Cleaner and polished training data
- Much higher accuracy and fluency
- Faster convergence and better generalization

### ğŸ”¹ Workflow
```
Dataset Polishing
      â†“
 Tokenization & Preprocessing
      â†“
 Fine-Tuning Pre-trained Model
      â†“
 Evaluation (â‰ˆ95%+ Accuracy)
      â†“
 API Deployment
```

### ğŸ”¹ Tech Stack
- Python
- Transformers (Hugging Face)
- PyTorch / TensorFlow
- FastAPI
- Hugging Face Spaces

### ğŸ”¹ Live API
ğŸ”— **Fine-Tuned Englishâ€“Hindi Translation API (High Accuracy)**  
ğŸ‘‰ https://patil1203-en-hi-translator-api.hf.space/

---


## ğŸ§  Key NLP Concepts Demonstrated
- Sequence-to-Sequence Learning
- Encoderâ€“Decoder Architecture
- LSTM Networks
- Tokenization & Padding
- Fine-Tuning Pre-trained Models
- Model Deployment as REST API
- End-to-End ML Product Development

---

## ğŸ“‚ Repository Structure (Suggested)
```
â”œâ”€â”€ project-1-lstm-encoder-decoder/
â”‚   â”œâ”€â”€ training_notebook.ipynb
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ tokenizer/
â”‚
â”œâ”€â”€ project-2-fine-tuned-model/
â”‚   â”œâ”€â”€ fine_tuning_notebook.ipynb
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ model/
â”‚
â””â”€â”€ README.md
```

---

## ğŸ¯ Learning Outcomes
- Built NMT models from scratch
- Understood limitations of LSTM-based translation
- Applied fine-tuning to improve NLP performance
- Deployed ML models as real-world APIs
- Used Hugging Face ecosystem effectively

---

## ğŸ™Œ Acknowledgements
- **IIT Bombay** for the Englishâ€“Hindi dataset
- **Hugging Face** for datasets, transformers & Spaces

---

## ğŸ“¬ Contact
If you have suggestions or want to collaborate:
- ğŸ’¼ LinkedIn: *Add your LinkedIn here*
- ğŸ“§ Email: *Add your email here*

---

â­ If you like this project, donâ€™t forget to star the repository!

